name: Scrape + Publish (GitHub Pages)

on:
  schedule:
    - cron: "*/5 * * * *" # every 5 minutes
  workflow_dispatch: {}

permissions:
  contents: write

concurrency:
  group: scrape-publish
  cancel-in-progress: false

jobs:
  scrape:
    runs-on: ubuntu-latest
    timeout-minutes: 12

    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Setup Node
        uses: actions/setup-node@v4
        with:
          node-version: 20
          cache: npm

      - name: Install dependencies
        run: npm ci

      - name: Install Playwright (Chromium)
        run: npx playwright install --with-deps chromium

      - name: Run scraper
        env:
          NODE_ENV: production
        run: node server.js --refresh

      - name: Copy cache.json to docs
        run: cp cache.json docs/cache.json

      - name: Verify output files exist
        run: |
          test -d docs || (echo "Missing docs/ directory" && exit 1)
          test -f docs/cache.json || (echo "Missing docs/cache.json" && exit 1)
          ls -la docs/

      - name: Commit & push updated data
        run: |
          git config user.name "newsboard-bot"
          git config user.email "newsboard-bot@users.noreply.github.com"

          git add -f docs/cache.json

          if git diff --cached --quiet; then
            echo "No changes to commit."
            exit 0
          fi

          git commit -m "Update scraped data"
          git push